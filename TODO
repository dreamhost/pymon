============
REQUIREMENTS
============

* Monitor first 100s then 1000s and then millions of resources without falling
  over.
* Provide a simple web iterface with query results that get updated dynamically
  (AJAX).
* Provide a clean and clear way for writing plugins.
* Put workflow at the core of all event-based heuristics.
* Support customization and subscriptions (requires user accounts).

Scalability
-----------

In order for pymon to be so highly scalable, the following need to be
satisfied:
 * No code must block (use deferreds!), including database code. Options:
   - use Axiom
   - use Storm with Twisted support
   - manually use queues and/or threads (deferToThread)

 * Service configuration (and anything else that needs to be confgured per
   host) need to be easily generated, updated, etc.

 * Current state info for each process needs to be accessible via a URL so the
   object publishing for that data also needs to be well enough thought-out
   that accessing 1000s to millions of monitored service states is "easy" and
   intuitive.

    Scalable Configuration (DONE)
    ----------------------

     * Scan directories and locate configuration files
       - load in order of increasing depth, sorted alpha-numericly
     * Load config files and amalgamate into a single string
     * Create a ZConfig object based on that string

    It seems that ZConfig.schemaless might be just the thing. This is somethig
    that would work well for plugins, too:

     * plugins could provide the defaults
     * etc/services/*.conf would be scanned for services
     * then all services of that type could be concatenated into a single
       string and a config "file" created from this

    Service configuration would be inserted into pymon.conf, replacing a string
    such as %(SERVICES)s.

    The content of that string would be built from scanning all plugin
    directories for offered services and then building appropriate ZConfig
    config file strings. For instance, for a service, the following template
    could be used:

        <service-name>
            %(DEFAULTS)s
            %(CHECKS)s
        </service-name>

    where %(DEFAULTS)s would have the following form:

        <service-name-defaults>
            key1 value1
            key2 value2
        </service-name-defaults>

    and %(CHECKS)s would be a group of notes with the following form:

        <service-name-check>
            uri     myhost1.com
            enabled true
        </service-name-check>
        <service-name-check>
            uri     myhost2.com
            enabled true
        </service-name-check>
        <service-name-check>
            uri     myhost3.com
            enabled true
        </service-name-check>

    The next question is how to scale etc/services/*.conf. The following are
    options, not necessarily mutually exclusive:

     * one huge single file for each service type (etc/services/ping.conf)
     * mutlple files that split groups of a single service over several files
       (etc/services/ping/*.conf)
     * one file for each service (etc/services/ping/x/y/z.conf); subdirectories
       would need to be automatically managed based on number (probably not
       more than 1000 files per directory); ideal for generated conf files, not
       manually managed service configuration

    All of those could actually be used together at the same time, as long as
    each file has not wrapping tags, only the service tags. For example, each
    file would have one or more of the following:

        <ping-check>
            key1 value1
            key2 value3
        </ping-check>

    This collection of service check configuration tags would be assembled,
    post-pended to the content of plugins/ping/defaults.conf (the contents
    would be the sibling tag, <ping-defaults>) and then wrapped with the <ping>
    service tag.

    Naming standards for directories under etc/services need to be strictly
    adhered to, otherwise user configuration will go overlooked or misread. All
    service configurations should follow this form:

     * etc/services/service-name.conf
     * etc/services/service-name/*.conf
     * etc/services/service-name/*/[*/...]*.conf

    For example:
     * etc/services/ping.conf
     * etc/services/ping/myhost.conf
     * etc/services/ping/myhosts.conf
     * etc/services/ping/group1.conf
     * etc/services/ping/group2.conf
     * etc/services/ping/EasternSeaboard/Financial/hosts.conf
     * etc/services/ping/WestCoast/Research/hosts.conf

    In addition to the current <service-name-defaults>, we are probably also
    going to need <service-name-all> for configuration options that apply to
    all of a given service type. This becomes necessary when moving defaults
    into the plugins, as:

     * it was previously conceived that the defaults would be manually edited
       by users to provide configuration options used by all services of that
       type, and
     * plugin code is not intended for user configuration

    Documentation needs to be supplied for plugin developers. Specifically,
    they would need to know about the following:

     * how to add notification lists (emails)
     * how to support maintenance windows
     * how to ... ?

    We will want to provide convenience scripts that can do something like the
    following:
     1. portscan a network
     2. create host files from the discovered hosts


    Scalabale Database Access (IN PROGRESS)
    -------------------------
    Some notes about intended direction:
     * Use relational DB for check data (monitored service state and history)
     * Database access has to be async in order to scale well with pymon
     * Programmatic interface has to be clean and object-oriented

    This means the following needs to take place:
     * DONE - Migrate to an async-capable ORM (Storm with Twistesd support)
     * DONE - Create datamodels
     * Store factories in the database instead of memory
        - factories need to be picklable
        - need to add 'factory' blob field to service
        - may need to refactor factories such that they can provide picklable
          bits (__getdata__?) and can recieve unpickled data (and update their
          states) before firing off a service check
     * All occurances of pymon.application.MonitorState need to be removed
        - this includes IState and related componentry
        - these need to be replaced with their storage counterparts
          (add/update/delete/etc data)
     * All occurances of pymon.application.History need to be removed


    Scalable JSON State URLs
    ------------------------
    We probably need to address URL layouts for all services. This includes the
    following:
     * Data presented in the web UI
     * JSON State Data
     * RSS
     * XLM-RPC
     * JSON-RPC
    The last two are actually pretty easy: we'll publish the APIs at BASE/RPC2
    and BASE/RPC3, respectively.

    The others, though, will need to be present at any node where that
    information would be pertinent, and will probably have the form of
    BASE/PATH, BASE/PATH/data, BASE/PATH/feed.

    It's the PATH that's the tricky bit. Possible permutations include:
     * BASE/node/HOSTORIP - all services for that node
     * BASE/node/HOSTORIP/SERVICETYPE - only service for that node that is of
       the given type
     * BASE/service/SERVICETYPE - all services of a given type
     * BASE/organization/ORGNAME - all nodes, all services
     * BASE/organization/ORGNAME/HOSTORIP - filtered by org
     * BASE/organization/ORGNAME/HOSTORIP/SERVICETYPE - filtered by org
     * BASE/organization/service/SERVICETYPE - all services of a given type

    This architecture needs to be able to support:
     * virtual hosting (e.g., fo isolating pymon views/data for an organization)
     * authentication than can be limited by PATH

Workflow
--------
 Currently:
 * Rules are 'built' when a monnitor client makes a connection (buildRules())
 * Rules are 'checked' when a monitor client connection is lost (sules.check())

 Needed:
 * Define an exact relationship between rules (x is WARN if status is between b
   and c) and workflow (perform tasks i and j when state transitions from m
   to n)

Here's how the flow should work:
 * Pre-workflow
   * monitor queries source
   * source responds
   * monitor parses returned data
 * Workflow
   * DONE - monnitor checks parsed data against expected data
   * monitor checks for state transitions
   * changes result in a state update and a history update
   * take action based on state transition

So, for workflow, there are a few separate but related bits of functionality:
 * threshold check
 * state comparison
 * state and history update
 * state transition-based actions

We need objects that represent these and allow them to interoperate in the
desired ways.
 * DONE - threshold check - a function should do the trick here: result in,
   OK|WARN|ERROR|FAIL out
 * DONE - state comparison - theshold check and last state in; workflow does this in
   doTrans()
   * we shouldn't actually enter doTrans() unless there is, in fact, a
     difference in current and previous state
   * we need to do more than compare states, e.g.,
   * detect flapping
 * state update - workflow does this in doTrans()
 * take action - probably in doTrans() as well as onEnter*() or onLeave*()
   * sending messages (needs to take into account message preferences like
     message count cut-offs)
   * escalating

Plugins
-------

 * Adding a new monitor to pymon needs to be easy and intuitive (for a python
   programmer familiar with Twisted, that is)
    - add HOWTO and tutorial docs to the sf.net website and/or google wiki
    - post to the mail list about the docs and the feature
 * DONE - Dispatching needs to work with configuration files and plugins, not just
   python code (pymon.monitors.AbstractFactory.makeMonitor)
 * DONE - All service monitors (even the bundled ones) should be in the plugins
   directory


AJAX
----

The service check stats need to be updated based on the frequency with which
each monitored service is checked. The UI should dynamically update when the
status data has been updated.

User Accounts, Subscriptions, and Messaging
-------------------------------------------

If we introduce the idea of users/accounts in pymon, we can have subscriptions.
People can choose the services for which they want to be notified in case of
any state transitions that warrant notification.

Hmm... each monitor needs to provide an RSS feed, allowing people to pull
allerts (as opposed to have alerts pushed to them via emails).

There is now some orphaned code that did the email messaging (orphaned after it
was split from the workflow rules code). This needs to be made functional
again... and with a mind to how we will be managing messaging in general (see
the RSS comment above).

Ah, another form of messaging: what about SMNP traps? A pymon MIB would define
this (if memory serves)... need to look at how SNMP "messaging" works.

Generalization of messenging:
 * create a service that listens for messages
 * all publishable messages will be sent to that service
 * need to pass information like:
    - message type
    - message content
    - service type
    - service host
    - **kwds for data particular to the message type, such as:
        . subject, to, from (for emails)
        . date, title, link (for RSS)
        . ? (for SNMP)

Listener
    The listener will be a local agent (pb.spread), much like the ping agent
    that is used for running the ping command line tool for host-checking.

    The listener will be responsible for making connections to remote servers
    (choosing the appropriate protocols, clients, etc.) after receiving a
    message from pymon.

    As such (and in addition), the listener will need to be an intelligent
    client factory, more or less.

Message Types
    Email:
        * in addition to the default, require parameters, takes to, from, and
       subject
        * needs to be able to connect to either sendmail or remote SMTP host

    Pages:
        * identical to email, except with special content formatting for phones

    RSS

    SNMP

    IRC

    AIM

    Twitter

    MUSH/MUD



========
New TODO
========

* add support for non-connection oriented monitor plugins
  - develop a means by which one can write plugins where deferreds can be
    "processed" for results in lieu of TCP connections
  - this will require fulfilling all the requirements that are currently met in
    plugins through the use of connectionMade and connectionLost; these need to
    be documented somewhere as part of the pymon monitor plugin API; these
    include, but may not be limited to the following:
     . setting up rules and workflow (ClientMixin.setup)
     . setting factory (monitor) data
     . parsing factory data
     . checking thresholds
     . initiating workflow transitions
     . updating state information
     . deleting rules object (ClientMixin.teardown)
  - replicating the process that currently happens with connections (making and
    losing them) will probably go something like this:
     . create a monitor that subclasses both from ClientMixin and BaseMonitor
     . implement a monitor __call__ method that calls its superclass __call__
       with connect=False
     . call ClientMixin.setup
     . in __call__, execute a method that returns a deferred
     . implement a callback that sets the monitor's (factory's) data attribute
     . add a callback that processes the data, checks thresholds, does workflow
       transitions, and updates state
     . call ClientMixin.teardown
* move monitor client code for rules, messaging and workflow into a ClientMixin
  method (it should be the same for every monitor)
* look into the possibility of ClientMixin and MonitorBase subclassing the same
  base class (maybe only having the 'host' property + set/gets?); or better
  yet, having client plugins subclass ClientMixin and pymon.plugin.Plugin and
  monitor plugins subclass MonitorBase and pymon.plugin.Plugin
* add note to readme or other doc that using an smtp server will likely result
  in the "from" field set by pymon (pymon@adytum.us) being overriden (by the
  smtp server).
* update local mail sender to use process factory instead of making a direct
  call to popen
* plug new messaging into workflow
  - DONE -create messaging rules class for notification checks and temporary storage
    of generated messages
  - DONE - in plugins, after general rules check, run message check (isSend) and
    generate messages
  - DONE - in plugins, to the workflow transition check method, pass data necessary
    for sending messages to the listener
  - create a method (onTrans) that will get called for all transitions and in
    that method, iterate the created messages, make a listener client
    connection, and send the message to the listener
  - create MessageFactory class that takes a type and **kwds as parameters, and
    then generates the appropriate Message instance (to be used by the
    MessagingRules class)
* add third-party libs
* create installer script for third-party setup/installation
* update the INSTALL instructions to reflect the recent changes
* add a __repr__ for tcp ping factory
* with those in place, update ping_tcp plugin with a "sub-factory" that works
  like the factory in the discoverServices script; changes of the code need to
  make sure the following happens:
  - multiple connectTCP() calls are made to the host being checked (one for
    each port)
  - each port check needs to fire an event if the results exceed the threshold
  - deferreds may need to be sequential to avoid flooding; this might be
    possible by using the twisted task coiterators with a small batch size
  - we need a couple factories:
    . one that's respondible for connecting to a port (and reporting on
      connection errors)
    . one that's responsible for a series of port connections (and reporting on
      the overall/averaged success); this is the one whose protocol will be the
      monitor "client" that is responsible for firing off the workflow upon
      connectionLost (in this case, the connectionLost will have to be
      simulated, or contingent upon the last port connection being lost)
    . TCPPingSeriesClient must be for one series of pings on a single port and
      not more; as such, we will need a third layer, one for all series of
      for a given host (all ports, all counts)
  - ugh. this isn't working. probably need to explore a different solution
* add a method to the ConfigCheck class called getTiered() (or something),
  taking an attribute name as a parameter; it will check the 'check' and
  'defaults' attrs, returned the preferred one first, if it finds it (setting
  the preferred attr should happen in a passed param, too)
* once that's done, find all the places in the code where check and defauts are
  checked, and call the method instead
* add SQL create statements for tables in the updated model.
* add relationships to data model tables
* add tests for new tables and relationships
* consider copying configuration data to database once config files are
  iterated and parsed (and remove config data from memory)
   - make sure that pymon.config.CheckConfig was pickleable
   - add a blob file to service data model for CheckConfig pickle data
   - would also need to make sure that updates to the configs on the
     filesystem would refresh the config data stored in the DB
   - a save to config changes would flush impacted db records
   - all pymon code config lookups would first search the database, if not
     found, would parse filesystem config files (thus anything that had been
     flushed would get reloaded the next time the config data was queried)
* move the globalRegistry factories into persistent storage (storm.twisted) to
  save on memory
* completely remove globalRegistry once storm orm code is in place
* maybe make BaseMonnitor.__call__ return a deferred for updating/saving data
  of the last state (storm.twisted) and then add that deferred to a deferred
  list in plugins that subclass is (i.e., all plugins)
* pymon shouldn't spawn thousands of connections unless specifically configured
  to do so; instead use task.cooperate to batch connections... not sure how
  this should be architected...
* The following should also be made to return deferreds:
    * plugins' connectionLost output parsing
    * ThresholdRules.check
    * ServiceState.checkTransition
    * plugins' connectionLost updateState
* fix status counts for service state (dict obj); they're currently not
  incrementing (looks like the problem is not with the incrementing, but rather
  with the state saving -- see pymon.monitors, line 58)
* reexamine and better define the onEnter/Trans/Leave methods required for
  pymon's workflow
* use bisect module to determine ranges more efficiently
* expose service resources
  * JSON state data for monitor
  * RSS feed for state transitions of a monitor
  * provide /services/<service type>/<hostname>/<JSON data>
  * provide /services/<service type>/<hostname>/<RSS feed>
  * provide /hosts/<hostname>/<service type>/<JSON data>
  * provide /hosts/<hostname>/<service type>/<RSS feed>
* pull messaging crap out of ThresholdCheck class
* is pymon.clients.BaseClient.updateState doing its job?
* clean up old crap in pymon.clients.
* add support for twisted-based TCP pinger.
* create a doc/wiki page on pymon plugins (what they are, what they're based
  on, how they work)
* create a doc/wiki on how to *write* a pymon plugin
* create a doc/wiki on how adaptors are used in pymon (e.g., IState)
* start work on defining RPC for pymon
* migrate to database-stored data (ditch pickled data).
  - DONE - prototype and test async code
  - design schema
  - replace all state update code to use pymon storage API (which uses storm)
  - replace all event/history update code to use pymon storage.
* give more consideration to the following features:
  - events (state transitions that meet the notification theshold requirements)
  - event hisory
  - event acknowledgement
  - services states web UI (AJAX, batching results, etc.)
  - front page/dashboard and graphs
* write a script that uses nmap to:
  1. locate hosts on a network
  2. identify running services for each host
  3. create service check configs for each service on each host
* examine the reasons for having pymon.utils.LocalTools.
* fix broken links in web ui (probelms with relative URLs?)
* DONE - add dispatching support
* DONE - add ./plugins to the python path
* DONE - add factory configuration options to config files
* DONE - create a pymon-dev mail list on google groups
* DONE - Fix the fact that only one check config is getting saved in the StringIO
  config.
* DONE - fix the *FromURI methods in the schemaless ZConfig wrapper to properly use
  self.services and self.default.
* DONE - clean up old crap in pymon.monitors.
* DONE - fix the newly exposed bugs in the workflow code (see the unit test results);
  the issue is likely at least in Workflow.addTrans where self.states is set by
  both state from and state to. The check for this could be wrong, too
  (WorkflowAware.doTrans).
* DONE - plug workflow into pymon
* DONE - services seem to stay in RECOVERING continuously, and this is
  obviously wrong
* DONE - look into BaseMonitor.__call__ more closely (see XXX comment).
* DONE - rename all occurances of reactor_params to reactorArgs.
* DONE - move maintenance window check into its own method, isMaintenance()
* DONE - move disabled check into its own method, isDisabled()
* DONE - move state-saving functionality into its own method
* DONE - add a parameter to BaseMonitor.__call__ (e.g., "call=True") that would
  allow subclasses to determine if BaseMonitor should call connectTCP or not
* DONE - write a twisted-based iservice discovery script (no nmap).
* DONE - rename ./admin scripts to conform to standards
* DONE - add support to the news-publishing script to convert SF.net news links of the
  form TEXT (URL), used for official SF.net news to HTML links used on the
  pymon web site.
* DONE - split utils/message into messaging/email and messaging/formatters
* DONE - extract general code from command-line pinger client and put into agents code
* DONE - write messaging listener (start by copying usable code from agents.local)
  that:
    - has a message() method that takes message type, message content, service
      type, service host, and **kwds
    - has client factories for each of the supported message types
* DONE - write an email messenger that:
    - can be instantiated by workflow when messages need to be sent
    - can connect to the listener and send messages

====
TODO (old)
====


State and Workflow overhaul
---------------------------
* DONE - Get rid of application.State's current use of backing up
    o DONE - have State subclass twisted.persisted.sob.Persistent
    o DONE - for regular backups, call monitorStateInstance.save()
        note: regular backups are called by a TimerService in the main loop
    o DONE - for restoring data, simply use twisted.persisted.sob.load()
* DONE - Rename application.State to application.BaseState (subclassing Persistent)
* DONE - Create application.CheckData, a dictionary with default values
* DONE - Plug rules processing into ServiceState methods
* DONE - Revamp application.MonitorState
    o DONE - __init__() instantiates a CheckState and assigns it to an attribute
    o DONE - checkStateInstance gets updated with all service changes; it's the 
        run-time data for the Monitor
* Upon monitors parsing/proccessing returned network data:
    o DONE - monitorClient.workflow.doTrans(this_transition_name)
    o doTran() will initiate the actions appropriate for that particular
      transition
* DONE - Convert all dictionary access to set/get access:

Workflow Customizations
-----------------------

o DONE - Move clients.rules.checkForMaintenanceWindow() to pymon.config.
o Implement a very basic flap-guard: don't send out a notice until you 
get x fails in a row, y warns in a row, or z errors in a row.
o With notification cut-offs in place, the next thing to implement
is checks for flapping services... that spend less than the cut-off
limit in each state.
o Run some prototype code with Glyph's Q2Q Vertext code... examine if 
it's possible to do this via ssh... setting up shared keys between pymon 
instances.
o Examine methodologies for performing the most checks in the
shortest time; calculate limitations of system (number of hosts,
time to check hosts, max checks in a given interval, etc.)
o Add pymon overview page to the web UI
o Add a form in the state lists for entering the number of seconds after 
which the page shoudl refresh.
o Convert the meta refrsh of the states web ui to AJAX/JSON.
o Fix email formatting (only a problem onn some servers... needs 
investigation)
o Add https checks
o Add organization to email message and subj; if not org, then host
or IP
o Add a built-in SMTP server for receiving acknowledgements from
users (mail list will be checked for "authorized users"; headers
will be checked for service uri)
o Add custom pymon mail headers to outgoing messages
o Add the ability to disable notifications on a per-service basis
o enable multiple criteria for process matching
o Fix the HTTP NullClient being __call__'ed.
o Make sending of emails in clients.rules.ThresholdRules.sendIt()
non-blocking.

o DONE - Make the loglevel (verbosity) configurable
o DONE - Use log levels for every log message (DEBUG, CRIT, INFO, etc.)
o DONE - Use JSON/AJAX for arbitrarily sorting by columns on the services
web views
o DONE - Fix the maintenance window notification skips; add "maintenance"
to state data structure
o DONE - Add a virtual host monster so that it can be easily integrated
with other web server software
o DONE - Fix the "disabled" service configuration option
o DONE - Add a regular check for config file modifications... re-load
config? This might mean changing the "backup server" to "admin
server", where backups and config file scanning/reloading would be
two of its tasks.
o DONE - Add failed message templates (these were actually already 
there for HTTP status checks, there was just a typo in the 
schema.xml file.
o DONE - Add notification threshold logic (cutoff based on count
to avoid getting spammed by pymon alerts)
o DONE - Fix the status processing for http_status -- it seems to
be broken
o DONE - Fix the Nevow web interface
o DONE - Get state data backups working for each monitor
o DONE - Fix exceptions with HTTP status failures
o DONE - create valid regex for human-readable ranged values in the
validator pymon.zconfig.rangedValues. This has been moved to an
externall library: adytum.config.zconfig.
o DONE - tweak the ZConfig schema for pymon configuration to go from this:
  <check_type-defaults>
    ...
  </check_type-defaults>
  <check_type>
    ...
  </check_type>
  <check_type>
    ...
  </check_type>

to something more like this:
  <check_type>
    <defaults>
      ...
    </defaults>
    <check>
      ...
    </check>
  </check_type>
o DONE - Get a minimal pymon shell working
o DONE - Entirely rip out configuration from globalRegistry -- there's no need 
to that to be copied to another object -- it's all available as a 
pymon.config import
o DONE - Start putting example configurations in the examples directory.
